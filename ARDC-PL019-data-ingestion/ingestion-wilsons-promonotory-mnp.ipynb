{"metadata": {"language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3 (ipykernel)", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "code", "source": "import boto3\nimport io\nimport urllib\nimport s3fs\nimport json\nfrom pathlib import Path\nimport attr\nimport numpy\nimport tiledb\nimport tiledb.cloud\nfrom tiledb.cloud.compute import DelayedArrayUDF, Delayed\nimport pandas\nimport geopandas\nimport fiona\nfrom fiona.session import AWSSession\nimport pystac\nfrom scipy.stats import skew, kurtosis\nimport uuid", "metadata": {"trusted": true}, "execution_count": 7, "outputs": [], "id": "a33fbec1-7325-4d5d-a617-aca3231b4e3e"}, {"cell_type": "code", "source": "import pystac\nfrom pystac.extensions.projection import ProjectionExtension\nfrom pystac.extensions.pointcloud import (\nPointcloudExtension,\nSchemaType,\nPhenomenologyType,\nSchema,\nStatistic,\n)", "metadata": {"trusted": true}, "execution_count": 8, "outputs": [], "id": "145bb56d-b429-48a0-a049-430bd944479a"}, {"cell_type": "code", "source": "from reap_gsf import reap, data_model\nfrom bathy_datasets import rhealpix, storage, geometry, asb_spreadsheet,stac_metadata", "metadata": {"trusted": true}, "execution_count": 9, "outputs": [], "id": "e134ee6b-2533-4b59-bd3a-e2f5c4198256"}, {"cell_type": "code", "source": "session = boto3.Session()\ncreds = session.get_credentials()", "metadata": {"trusted": true}, "execution_count": 10, "outputs": [], "id": "e8e1d669-0bd1-43fe-9745-7fc26b2f99de"}, {"cell_type": "code", "source": "fs = s3fs.S3FileSystem(key=creds.access_key, secret=creds.secret_key, use_listings_cache=False)", "metadata": {"trusted": true}, "execution_count": 11, "outputs": [], "id": "5cdf7a21-3da1-4ee8-aef3-ecedbd01d512"}, {"cell_type": "code", "source": "uid = uuid.uuid4()", "metadata": {"trusted": true}, "execution_count": 12, "outputs": [], "id": "3ec86796-69c5-44bd-974a-ae399afbd43f"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "d46629ff-a275-4e5a-8df5-9fe998160b8e"}, {"cell_type": "code", "source": "survey_uri = \"s3://ausseabed-pl019-provided-data/DeakinUniversity/WilsonsPromontory_MNP/\"\noutdir_uri = \"s3://ausseabed-pl019-ingested-data/L2/WilsonsPromontory_MNP/\"\nasb_metadata_uri = \"s3://ausseabed-pl019-provided-data/DeakinUniversity/WilsonsPromontory_MNP/metadata/spreadsheet-metadata.json\"\nsurvey_info_uri = \"s3://ausseabed-pl019-provided-data/DeakinUniversity/WilsonsPromontory_MNP/schema-info.json\"", "metadata": {"trusted": true}, "execution_count": 13, "outputs": [], "id": "63ede6e1-61d6-4b9c-a1e2-865a15322178"}, {"cell_type": "code", "source": "base_prefix = \"ga_ausseabed\"\narray_name = f\"{base_prefix}_{uid}_bathymetry\"\narray_uri = f\"{outdir_uri}{array_name}.tiledb\"\ntiledb_array_uri = f\"tiledb://sixy6e/{array_name}\"\nsoundings_cell_density_uri = f\"{outdir_uri}{base_prefix}_{uid}_soundings-cell-density-resolution-12.geojson\"\ncoverage_uri = f\"{outdir_uri}{base_prefix}_{uid}_coverage.geojson\"\nstac_md_uri = f\"{outdir_uri}{base_prefix}_{uid}_stac-metadata.geojson\"", "metadata": {"trusted": true}, "execution_count": 14, "outputs": [], "id": "c0ef54b8-e1f3-4eca-864e-cfa3ccbb6847"}, {"cell_type": "code", "source": "soundings_cell_density_uri_15 = f\"{outdir_uri}{base_prefix}_{uid}_soundings-cell-density-resolution-15.geojson\"", "metadata": {"trusted": true}, "execution_count": 15, "outputs": [], "id": "ee1d582e-f3f3-4224-9340-d3c5025060a4"}, {"cell_type": "code", "source": "def get_sonar_metadata(json_uri):\n    \"\"\"\n    Temporary func for pulling metadata from a sample GSF file.\n    \"\"\"\n    with fs.open(json_uri) as src:\n        md = json.loads(src.read())\n    stream_task = Delayed(\"sixy6e/retrieve_stream\", name=\"retrieve\")(md[\"gsf_uri\"], creds.access_key, creds.secret_key)\n    dataframe_task = Delayed(\"sixy6e/decode_gsf\", name=\"decode\", image_name=\"3.7-geo\")(stream_task, slice(10))\n    df, finfo = dataframe_task.compute()\n    sonar_metadata = finfo[3].record(0).read(stream_task.result()[0])\n    history = attr.asdict(finfo[6].record(0).read(stream_task.result()[0]))\n    for key, value in history.items():\n        sonar_metadata[key] = value\n    return sonar_metadata\n\n\ndef reduce_region_codes(results):\n    \"\"\"\n    The reduce part of the map-reduce construct for handling the region_code counts.\n    Combine all the region_code counts then summarise the results.\n    \"\"\"\n    region_codes = [i[0] for i in results]\n    timestamps = [i[1] for i in results]\n    df = pandas.concat(region_codes)\n    cell_count = df.groupby([\"region_code\"])[\"count\"].agg(\"sum\").to_frame(\"count\").reset_index()\n    \n    timestamps_df = pandas.DataFrame(\n        {\n            \"start_datetime\": [i[0] for i in timestamps],\n            \"end_datetime\": [i[1] for i in timestamps],\n        }\n    )\n\n    start_end_timestamp = [\n        timestamps_df.start_datetime.min().to_pydatetime(),\n        timestamps_df.end_datetime.max().to_pydatetime(),\n    ]\n\n    return cell_count, start_end_timestamp\n\n\ndef gather_stats(results):\n    \"\"\"\n    Gather the results from all the stats tasks and\n    combine into a single dict.\n    \"\"\"\n    data = {}\n    for item in results:\n        for key in item:\n            data[key] = item[key]\n    return data", "metadata": {"trusted": true}, "execution_count": 16, "outputs": [], "id": "77a8699b-81c1-47c3-80d7-1d3f39b4a36a"}, {"cell_type": "code", "source": "def retrieve_stream(uri, access_key, skey):\n    \"\"\"\n    Not testing the creation of the stream object at this point.\n    But for testing, we also need to keep the download to occur only\n    once.\n    \"\"\"\n    session = boto3.Session(aws_access_key_id=access_key, aws_secret_access_key=skey)\n    dev_resource = session.resource(\"s3\")\n    uri = urllib.parse.urlparse(uri)\n    obj = dev_resource.Object(bucket_name=uri.netloc, key=uri.path[1:])\n    stream = io.BytesIO(obj.get()[\"Body\"].read())\n    return stream, obj.content_length\n\n\ndef append_ping_dataframe(dataframe, array_uri, access_key, skey):\n    \"\"\"Append the ping dataframe read from a GSF file.\"\"\"\n    config = tiledb.Config(\n        {\"vfs.s3.aws_access_key_id\": access_key, \"vfs.s3.aws_secret_access_key\": skey}\n    )\n    ctx = tiledb.Ctx(config=config)\n    kwargs = {\n        \"mode\": \"append\",\n        \"sparse\": True,\n        \"ctx\": ctx,\n    }\n\n    tiledb.dataframe_.from_pandas(array_uri, dataframe, **kwargs)\n\n\ndef ingest_gsf_slice(\n    file_record, stream, access_key, skey, array_uri, idx=slice(None)\n):\n    \"\"\"\n    General steps:\n    Extract the ping data.\n    Calculate the rHEALPIX code.\n    Summarise the rHEALPIX codes (frequency count).\n    Get timestamps of first and last pings.\n    Write the ping data to a TileDB array.\n    res = [df.groupby([\"key\"])[\"key\"].agg(\"count\").to_frame(\"count\").reset_index() for i in range(3)]\n    df2 = pandas.concat(res)\n    df2.groupby([\"key\"])[\"count\"].agg(\"sum\")\n    \"\"\"\n    swath_pings = data_model.SwathBathymetryPing.from_records(file_record, stream, idx)\n    swath_pings.ping_dataframe[\"region_code\"] = rhealpix.rhealpix_code(\n        swath_pings.ping_dataframe.X, swath_pings.ping_dataframe.Y, 15\n    )\n\n    # frequency of dggs cells\n    cell_count = (\n        swath_pings.ping_dataframe.groupby([\"region_code\"])[\"region_code\"]\n        .agg(\"count\")\n        .to_frame(\"count\")\n        .reset_index()\n    )\n\n    start_end_time = [\n        swath_pings.ping_dataframe.timestamp.min().to_pydatetime(),\n        swath_pings.ping_dataframe.timestamp.max().to_pydatetime(),\n    ]\n\n    # write to tiledb array\n    append_ping_dataframe(swath_pings.ping_dataframe, array_uri, access_key, skey)\n\n    return cell_count, start_end_time\n\n\ndef ingest_gsf_slices(gsf_uri, access_key, skey, array_uri, slices):\n    \"\"\"\n    Ingest a list of ping slices from a given GSF file.\n    \"\"\"\n    stream, stream_length = retrieve_stream(gsf_uri, access_key, skey)\n    finfo = reap.file_info(stream, stream_length)\n    ping_file_record = finfo[1]\n\n    cell_counts = []\n    start_end_timestamps = []\n\n    for idx in slices:\n        count, start_end_time = ingest_gsf_slice(\n            ping_file_record, stream, access_key, skey, array_uri, idx\n        )\n        cell_counts.append(count)\n        start_end_timestamps.append(start_end_time)\n\n    # aggreate the ping slices and calculate the cell counts\n    concatenated = pandas.concat(cell_counts)\n    cell_count = (\n        concatenated.groupby([\"region_code\"])[\"count\"]\n        .agg(\"sum\")\n        .to_frame(\"count\")\n        .reset_index()\n    )\n\n    # aggregate the min and max timestamps, then find the min max timestamps\n    timestamps_df = pandas.DataFrame(\n        {\n            \"start_datetime\": [i[0] for i in start_end_timestamps],\n            \"end_datetime\": [i[1] for i in start_end_timestamps],\n        }\n    )\n\n    start_end_timestamp = [\n        timestamps_df.start_datetime.min().to_pydatetime(),\n        timestamps_df.end_datetime.max().to_pydatetime(),\n    ]\n\n    return cell_count, start_end_timestamp", "metadata": {"trusted": true}, "execution_count": 17, "outputs": [], "id": "8ce45fa0-868a-4b3f-95f8-74ce728ed975"}, {"cell_type": "code", "source": "def scatter(iterable, n):\n    \"\"\"\n    Evenly scatters an interable by `n` blocks.\n    Sourced from:\n    http://stackoverflow.com/questions/2130016/splitting-a-list-of-arbitrary-size-into-only-roughly-n-equal-parts\n\n    :param iterable:\n        An iterable or preferably a 1D list or array.\n\n    :param n:\n        An integer indicating how many blocks to create.\n\n    :return:\n        A `list` consisting of `n` blocks of roughly equal size, each\n        containing elements from `iterable`.\n    \"\"\"\n\n    q, r = len(iterable) // n, len(iterable) % n\n    res = (iterable[i * q + min(i, r) : (i + 1) * q + min(i + 1, r)] for i in range(n))\n    return list(res)", "metadata": {"trusted": true}, "execution_count": 18, "outputs": [], "id": "b72411a4-82e1-4195-8643-991a7d3090aa"}, {"cell_type": "code", "source": "def ingest_gsfs(files, size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node):\n    \"\"\"\n    Prototype ingester.\n    \"\"\"\n\n    node_counter = 0\n    skipped_files = []\n    large_files = []\n    tasks = []\n    tasks_dict = {n: [] for n in range(processing_node_limit)}\n\n    for pathname in files:\n        metadata_pathname = pathname.replace(\".gsf\", \".json\")\n        base_name = Path(pathname).stem\n        with fs.open(metadata_pathname) as src:\n            gsf_metadata = json.loads(src.read())\n\n        if (gsf_metadata[\"size\"] / 1024 / 1024) > size_limit_mb:\n            large_files.append(pathname)\n            continue\n\n        ping_count = gsf_metadata[\"file_record_types\"][\"GSF_SWATH_BATHYMETRY_PING\"][\"record_count\"]\n        if ping_count == 0:\n            skipped_files.append(pathname)\n            continue\n\n        slices = [slice(start, start+ping_slice_step) for start in numpy.arange(0, ping_count, ping_slice_step)]\n        slice_chunks = [slices[i:i+slices_per_node] for i in range(0, len(slices), slices_per_node)]\n\n        for slice_chunk in slice_chunks:\n            start_idx = slice_chunk[0].start\n            end_idx = slice_chunk[0].stop\n            task_name = f\"{base_name}-{start_idx}-{end_idx}-{node_counter}\"\n            task = Delayed(\"sixy6e/ingest_gsf_slices\", name=task_name, image_name=\"3.7-geo\")(gsf_metadata[\"gsf_uri\"], creds.access_key, creds.secret_key, array_uri, slice_chunk)\n            task.set_timeout(1800)\n\n            if len(tasks_dict[node_counter]):\n                task.depends_on(tasks_dict[node_counter][-1])\n\n            tasks.append(task)\n            tasks_dict[node_counter].append(task)\n            node_counter += 1\n\n            if node_counter == processing_node_limit:\n                node_counter = 0\n\n    reduce_task = Delayed(reduce_region_codes, \"reduce-region_codes-timestamps\", local=True)(tasks)\n    \n    return reduce_task, skipped_files, large_files", "metadata": {"trusted": true}, "execution_count": 19, "outputs": [], "id": "f6e16dbb-79cc-4653-86f3-d29697dc5945"}, {"cell_type": "code", "source": "def ingest_gsfs_local(files, size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node):\n    \"\"\"\n    Prototype ingester.\n    \"\"\"\n\n    node_counter = 0\n    skipped_files = []\n    large_files = []\n    tasks = []\n    tasks_dict = {n: [] for n in range(processing_node_limit)}\n\n    for pathname in files:\n        metadata_pathname = pathname.replace(\".gsf\", \".json\")\n        base_name = Path(pathname).stem\n        with fs.open(metadata_pathname) as src:\n            gsf_metadata = json.loads(src.read())\n\n        #if (gsf_metadata[\"size\"] / 1024 / 1024) > size_limit_mb:\n        #    large_files.append(pathname)\n        #    continue\n\n        ping_count = gsf_metadata[\"file_record_types\"][\"GSF_SWATH_BATHYMETRY_PING\"][\"record_count\"]\n        if ping_count == 0:\n            skipped_files.append(pathname)\n            continue\n\n        slices = [slice(start, start+ping_slice_step) for start in numpy.arange(0, ping_count, ping_slice_step)]\n        slice_chunks = [slices[i:i+slices_per_node] for i in range(0, len(slices), slices_per_node)]\n\n        for slice_chunk in slice_chunks:\n            start_idx = slice_chunk[0].start\n            end_idx = slice_chunk[0].stop\n            task_name = f\"{base_name}-{start_idx}-{end_idx}-{node_counter}\"\n            task = Delayed(ingest_gsf_slices, name=task_name, local=True)(gsf_metadata[\"gsf_uri\"], creds.access_key, creds.secret_key, array_uri, slice_chunk)\n            task.set_timeout(1800)\n\n            if len(tasks_dict[node_counter]):\n                task.depends_on(tasks_dict[node_counter][-1])\n\n            tasks.append(task)\n            tasks_dict[node_counter].append(task)\n            node_counter += 1\n\n            if node_counter == processing_node_limit:\n                node_counter = 0\n\n    reduce_task = Delayed(reduce_region_codes, \"reduce-region_codes-timestamps\", local=True)(tasks)\n    \n    return reduce_task, skipped_files, large_files", "metadata": {"trusted": true}, "execution_count": 20, "outputs": [], "id": "3f6e67ce-d214-406a-bc24-ae67fee4a109"}, {"cell_type": "code", "source": "with fs.open(survey_info_uri) as src:\n    survey_info = json.loads(src.read())", "metadata": {"trusted": true}, "execution_count": 21, "outputs": [], "id": "15d536ca-a780-41ad-a41a-1b19fc0850a1"}, {"cell_type": "code", "source": "#required_attributes = survey_info[\"schemas\"][0]\n# this is temporary. better to have it defined internally. or programmatically derived as a union of all schemas from all pings\nrequired_attributes = [\n    \"Z\",\n    \"across_track\",\n    \"along_track\",\n    \"beam_angle\",\n    \"beam_angle_forward\",\n    \"beam_flags\",\n    \"beam_number\",\n    \"centre_beam\",\n    \"course\",\n    \"depth_corrector\",\n    \"gps_tide_corrector\",\n    \"heading\",\n    \"heave\",\n    \"height\",\n    \"horizontal_error\",\n    \"ping_flags\",\n    \"pitch\",\n    \"roll\",\n    \"sector_number\",\n    \"separation\",\n    \"speed\",\n    \"tide_corrector\",\n    \"timestamp\",\n    \"travel_time\",\n    \"vertical_error\",\n    \"region_code\",\n]", "metadata": {"trusted": true}, "execution_count": 22, "outputs": [], "id": "0e4fa6d0-ffcc-4a00-9b7b-08669cff4fef"}, {"cell_type": "code", "source": "config = tiledb.Config(\n        {\"vfs.s3.aws_access_key_id\": creds.access_key, \"vfs.s3.aws_secret_access_key\": creds.secret_key}\n    )\nconfig_dict = config.dict()\nctx = tiledb.Ctx(config=config)", "metadata": {"trusted": true}, "execution_count": 23, "outputs": [], "id": "3a14176c-b3f6-4cf3-b26c-74962b18bff7"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "1607799b-5b4b-4ab5-9f7d-f4180b7ac1d4"}, {"cell_type": "code", "source": "storage.create_mbes_array(array_uri, required_attributes, ctx)", "metadata": {}, "execution_count": null, "outputs": [], "id": "182ac795-a44e-42f3-9917-a91c1a86cfd2"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "576aec12-e135-4c79-9a70-30e9491e8a30"}, {"cell_type": "code", "source": "files = fs.glob(survey_uri + \"**.gsf\")\nlen(files)", "metadata": {"trusted": true}, "execution_count": 24, "outputs": [{"execution_count": 24, "output_type": "execute_result", "data": {"text/plain": "705"}, "metadata": {}}], "id": "8cf67d53-0630-4123-b9b7-6ec440cfa738"}, {"cell_type": "code", "source": "sonar_metadata = get_sonar_metadata(files[0].replace(\".gsf\", \".json\"))", "metadata": {"trusted": true}, "execution_count": 25, "outputs": [], "id": "3b423599-8df7-4387-91fb-33a5002b30e8"}, {"cell_type": "code", "source": "n_partitions = 8\nfiles_blocks = scatter(files, n_partitions)\nlen(files_blocks[0])", "metadata": {"trusted": true}, "execution_count": 26, "outputs": [{"execution_count": 26, "output_type": "execute_result", "data": {"text/plain": "89"}, "metadata": {}}], "id": "de67884c-7baa-4ee5-b326-996abdca792e"}, {"cell_type": "code", "source": "size_limit_mb = 500\nprocessing_node_limit = 5\nping_slice_step = 2000\nslices_per_node = 3\nlocal_tasks_limit = 1\nlocal_ping_slice_step = 2000\nlocal_slices_per_task = 4", "metadata": {"trusted": true}, "execution_count": 27, "outputs": [], "id": "c57f9186-92a7-493b-8744-7c7dbb36e37a"}, {"cell_type": "code", "source": "skipped_files = []\nlarge_files = []", "metadata": {"trusted": true}, "execution_count": 28, "outputs": [], "id": "9acb138f-2e55-40c8-b3aa-5a959be44ae0"}, {"cell_type": "code", "source": "reduce_task, skipped_files1, large_files1 = ingest_gsfs(files_blocks[0], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "304196a7-65a3-4e60-b5d5-4e78fcb11b0d"}, {"cell_type": "code", "source": "cell_count_df1, start_end_timestamps1 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "6ce0312a-cfce-4b7d-bafb-830f5985305b"}, {"cell_type": "code", "source": "reduce_task, skipped_files2, large_files2 = ingest_gsfs(files_blocks[1], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "df59ef1c-bef2-40e3-b26f-ea7088755570"}, {"cell_type": "code", "source": "cell_count_df2, start_end_timestamps2 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "e333c9ac-997a-4727-8413-21e6cd817ac6"}, {"cell_type": "code", "source": "reduce_task, skipped_files3, large_files3 = ingest_gsfs(files_blocks[2], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "7d77ede4-3632-48bb-ba55-0dc3a5d1d0b0"}, {"cell_type": "code", "source": "cell_count_df3, start_end_timestamps3 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "157bf8fd-c6fd-4db0-a4e1-267bec33fab1"}, {"cell_type": "code", "source": "reduce_task, skipped_files4, large_files4 = ingest_gsfs(files_blocks[2], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "39ddb95d-e44b-44a9-b36f-9aadccdf8441"}, {"cell_type": "code", "source": "cell_count_df4, start_end_timestamps4 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "000eeac0-a79f-4d6e-add0-79320c0a9d5d"}, {"cell_type": "code", "source": "reduce_task, skipped_files5, large_files5 = ingest_gsfs(files_blocks[4], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "6f818fcb-a35e-43a3-b140-2b98bb8626e6"}, {"cell_type": "code", "source": "cell_count_df5, start_end_timestamps5 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "a9526e76-9189-4b1a-843b-d58bc8c51a91"}, {"cell_type": "code", "source": "reduce_task, skipped_files6, large_files6 = ingest_gsfs(files_blocks[5], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "3a6052ef-68cf-40a9-8d56-992733f84dbd"}, {"cell_type": "code", "source": "cell_count_df6, start_end_timestamps6 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "96cc1e87-6f8e-421f-8433-94d876cdd892"}, {"cell_type": "code", "source": "reduce_task, skipped_files7, large_files7 = ingest_gsfs(files_blocks[6], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "0a178d69-d80b-4612-810b-f1b15ac016fc"}, {"cell_type": "code", "source": "cell_count_df7, start_end_timestamps7 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "865ae250-1ef3-4b89-a347-e41a25a84ddd"}, {"cell_type": "code", "source": "reduce_task, skipped_files8, large_files8 = ingest_gsfs(files_blocks[7], size_limit_mb, processing_node_limit, ping_slice_step, slices_per_node)", "metadata": {}, "execution_count": null, "outputs": [], "id": "1b0bad98-f39c-4ef5-8283-ff178ce3e8f5"}, {"cell_type": "code", "source": "cell_count_df8, start_end_timestamps8 = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "7c4de5b0-180d-40d2-819d-0f96d0ecf4f9"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "8a23d6a5-dcd2-4167-b359-3b4fbf9f5fcf"}, {"cell_type": "code", "source": "large_files.extend(large_files1)\nlarge_files.extend(large_files2)\nlarge_files.extend(large_files3)\nlarge_files.extend(large_files4)\nlarge_files.extend(large_files5)\nlarge_files.extend(large_files6)\nlarge_files.extend(large_files7)\nlarge_files.extend(large_files8)", "metadata": {}, "execution_count": null, "outputs": [], "id": "eb07b496-f8e2-41f3-af95-653178771f3e"}, {"cell_type": "code", "source": "skipped_files.extend(skipped_files1)\nskipped_files.extend(skipped_files2)\nskipped_files.extend(skipped_files3)\nskipped_files.extend(skipped_files4)\nskipped_files.extend(skipped_files5)\nskipped_files.extend(skipped_files6)\nskipped_files.extend(skipped_files7)\nskipped_files.extend(skipped_files8)", "metadata": {}, "execution_count": null, "outputs": [], "id": "87e7b172-a707-4986-bc0b-10b2d173e234"}, {"cell_type": "code", "source": "len(large_files)", "metadata": {}, "execution_count": null, "outputs": [], "id": "46c8a2f0-878c-45d0-80d5-916915f1abb6"}, {"cell_type": "code", "source": "len(skipped_files)", "metadata": {}, "execution_count": null, "outputs": [], "id": "7ebe0209-28ce-47fe-8d08-eba66d31d570"}, {"cell_type": "code", "source": "reduce_task, skipped_files_local, large_files_local = ingest_gsfs_local(large_files, size_limit_mb, local_tasks_limit, local_ping_slice_step, local_slices_per_task)", "metadata": {}, "execution_count": null, "outputs": [], "id": "66616397-44b3-4cd5-a1a6-c9c6a126e50f"}, {"cell_type": "code", "source": "reduce_task.visualize()", "metadata": {}, "execution_count": null, "outputs": [], "id": "7b43c7c2-c7f7-445c-8ee9-540b0be94842"}, {"cell_type": "code", "source": "cell_count_df_local, start_end_timestamps_local = reduce_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "4c4b535a-188d-4f12-806d-fbb214d4b685"}, {"cell_type": "code", "source": "# collect and reduce dataframes and ping start end times", "metadata": {}, "execution_count": null, "outputs": [], "id": "bea84ef6-c97f-4e54-8e0b-6f31286a87bd"}, {"cell_type": "code", "source": "local_non_local_results = [\n    [cell_count_df1, start_end_timestamps1],\n    [cell_count_df2, start_end_timestamps2],\n    [cell_count_df3, start_end_timestamps3],\n    [cell_count_df4, start_end_timestamps4],\n    [cell_count_df5, start_end_timestamps5],\n    [cell_count_df6, start_end_timestamps6],\n    [cell_count_df7, start_end_timestamps7],\n    [cell_count_df8, start_end_timestamps8],\n    [cell_count_df_local, start_end_timestamps_local],\n]", "metadata": {}, "execution_count": null, "outputs": [], "id": "9ea98b11-c40a-4ca6-b28a-79f770cb02e6"}, {"cell_type": "code", "source": "final_cell_count_df, final_start_end_timestamps = reduce_region_codes(local_non_local_results)", "metadata": {}, "execution_count": null, "outputs": [], "id": "cb23a220-4ae1-4d4c-8619-b3f292679b1d"}, {"cell_type": "code", "source": "final_cell_count_df", "metadata": {}, "execution_count": null, "outputs": [], "id": "1d40a458-8fad-45cc-8649-53bc95bb1104"}, {"cell_type": "code", "source": "final_start_end_timestamps", "metadata": {}, "execution_count": null, "outputs": [], "id": "f8fdf45a-fb25-45d6-b6a2-3440e34c9af6"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "5b6c1767-e04c-4867-a305-fe8284cae964"}, {"cell_type": "code", "source": "final_cell_count_df[\"geometry\"] = rhealpix.rhealpix_geo_boundary(final_cell_count_df.region_code.values)", "metadata": {}, "execution_count": null, "outputs": [], "id": "4a446a6d-59ac-45d1-a809-a7f0e5cd2fca"}, {"cell_type": "code", "source": "gdf_15 = geopandas.GeoDataFrame(final_cell_count_df, crs=\"epsg:4326\")", "metadata": {}, "execution_count": null, "outputs": [], "id": "a9d41c25-a6c7-4a76-ba2e-7baad487ae3c"}, {"cell_type": "code", "source": "with fiona.Env(session=AWSSession(aws_access_key_id=creds.access_key, aws_secret_access_key=creds.secret_key)):\n    gdf_15.to_file(soundings_cell_density_uri_15, driver=\"GeoJSONSeq\", coordinate_precision=11)", "metadata": {}, "execution_count": null, "outputs": [], "id": "c6a2161a-b9a6-4a90-b67b-1a4a09c672c6"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "9c47a4f5-f1b1-4d47-b09d-c1cb1ace87e0"}, {"cell_type": "code", "source": "resolution12_df = pandas.DataFrame(\n    {\n        \"region_code\": final_cell_count_df.region_code.str[0:13],\n        \"count\": final_cell_count_df[\"count\"],\n    }\n).groupby(\n    [\"region_code\"]\n)[\"count\"].agg(\"sum\").to_frame(\"count\").reset_index()", "metadata": {}, "execution_count": null, "outputs": [], "id": "03885ae8-0889-49ca-a56e-d16d61135e79"}, {"cell_type": "code", "source": "resolution12_df", "metadata": {}, "execution_count": null, "outputs": [], "id": "a32f455e-d3a4-41af-b251-68df6b2752b7"}, {"cell_type": "code", "source": "resolution12_df[\"geometry\"] = rhealpix.rhealpix_geo_boundary(resolution12_df.region_code.values)", "metadata": {}, "execution_count": null, "outputs": [], "id": "1c08d440-2fad-4ff1-a479-cc2712a23aa0"}, {"cell_type": "code", "source": "gdf = geopandas.GeoDataFrame(resolution12_df, crs=\"epsg:4326\")", "metadata": {}, "execution_count": null, "outputs": [], "id": "0834e68b-ba4b-46be-b4a1-355c0e7db9ce"}, {"cell_type": "code", "source": "with fiona.Env(session=AWSSession(aws_access_key_id=creds.access_key, aws_secret_access_key=creds.secret_key)):\n    gdf.to_file(soundings_cell_density_uri, driver=\"GeoJSONSeq\", coordinate_precision=11)", "metadata": {}, "execution_count": null, "outputs": [], "id": "d74fa3ab-e77b-42aa-b842-e68ddc3c25e1"}, {"cell_type": "code", "source": "dissolved = geopandas.GeoDataFrame(geometry.dissolve(gdf), crs=\"epsg:4326\")", "metadata": {}, "execution_count": null, "outputs": [], "id": "5e1a747f-885a-46d2-9f24-0c3c37ed9df3"}, {"cell_type": "code", "source": "with fiona.Env(session=AWSSession(aws_access_key_id=creds.access_key, aws_secret_access_key=creds.secret_key)):\n    dissolved.to_file(coverage_uri, driver=\"GeoJSONSeq\", coordinate_precision=11)", "metadata": {}, "execution_count": null, "outputs": [], "id": "d77ab918-60bd-41ae-8952-a5a27a5d48a4"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "b8debfab-4b29-47fd-89d4-79c326d7f4a4"}, {"cell_type": "code", "source": "dggs = rhealpix.RhealpixDGGS.from_ellipsoid()", "metadata": {}, "execution_count": null, "outputs": [], "id": "72cb9f7b-04f5-4945-b28a-807fda036182"}, {"cell_type": "code", "source": "dggs.cell_width(12)", "metadata": {}, "execution_count": null, "outputs": [], "id": "4a1a1138-a748-4e0e-8943-39de456c6020"}, {"cell_type": "code", "source": "area_ha = gdf.shape[0] * dggs.cell_width(12) **2 / 10000\nsonar_metadata[\"area_ha\"] = area_ha\narea_ha", "metadata": {}, "execution_count": null, "outputs": [], "id": "5a33effd-77fe-476e-87f4-eef73251faec"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "72e2e114-70f6-4e3d-847c-31fd28bf4423"}, {"cell_type": "code", "source": "with fs.open(asb_metadata_uri) as src:\n    asb_metadata = json.loads(src.read())", "metadata": {}, "execution_count": null, "outputs": [], "id": "b00429b8-17ff-4d82-9230-ff2892b70eb6"}, {"cell_type": "code", "source": "tiledb.cloud.register_array(\n    uri=array_uri,\n    namespace=\"sixy6e\", # Optional, you may register it under your username, or one of your organizations\n    array_name=array_name,\n    description=asb_metadata[\"survey_general\"][\"abstract\"],  # Optional \n    access_credentials_name=\"AusSeabedGMRT-PL019\"\n)", "metadata": {}, "execution_count": null, "outputs": [], "id": "75c2350b-dfb0-4f1e-aa68-f8472b5f864e"}, {"cell_type": "code", "source": "with tiledb.open(array_uri, ctx=ctx) as ds:\n    schema = ds.schema\n    domain = ds.domain\n    non_empty_domain = ds.nonempty_domain()", "metadata": {}, "execution_count": null, "outputs": [], "id": "6e41bc2d-80f8-4eda-9477-dc6d66c43b94"}, {"cell_type": "code", "source": "gdf[\"count\"].max()", "metadata": {}, "execution_count": null, "outputs": [], "id": "a9ef7b67-0383-491b-941f-f46f0f1e7a79"}, {"cell_type": "code", "source": "gdf[\"count\"].min()", "metadata": {}, "execution_count": null, "outputs": [], "id": "d41ed80f-7783-4b30-b147-61ee8de2bfc4"}, {"cell_type": "code", "source": "full_idx = (slice(*non_empty_domain[0]), slice(*non_empty_domain[1]))\nfull_idx", "metadata": {}, "execution_count": null, "outputs": [], "id": "ccc55413-2274-4c1a-83a7-eefa7f661343"}, {"cell_type": "code", "source": "# test first to see if stats can be generated with full domain, or if we need to iterate over region codes\n# use the X from the schema. This should use the most memory. if it fails then use the scatter approach", "metadata": {}, "execution_count": null, "outputs": [], "id": "64378371-8d37-4744-92a4-4bed752b0b43"}, {"cell_type": "code", "source": "# task = Delayed(\"sixy6e/basic_statistics_incremental\", name=\"test-X-stat-full-idx\")(array_uri, config_dict, \"X\", schema=\"X\", idxs=[full_idx], summarise=True)\n# result = task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "160bd589-db04-4fac-a339-3ee2db86b2b2"}, {"cell_type": "code", "source": "# reduce the region code resolution", "metadata": {}, "execution_count": null, "outputs": [], "id": "71511e76-d2b6-4a64-97b1-34f7d038be04"}, {"cell_type": "code", "source": "gdf2 = geopandas.GeoDataFrame({\"region_code\": gdf.region_code.str[0:11], \"count\": gdf[\"count\"]}).groupby([\"region_code\"])[\"count\"].agg(\"sum\").to_frame(\"count\").reset_index()", "metadata": {}, "execution_count": null, "outputs": [], "id": "3fced06b-adff-451b-9f88-dce39b36b1ff"}, {"cell_type": "code", "source": "gdf2", "metadata": {}, "execution_count": null, "outputs": [], "id": "ad1e0cd1-c8d7-43be-90a7-4d5b4881fc0d"}, {"cell_type": "code", "source": "slices = []\nfor geom in rhealpix.rhealpix_geo_boundary(gdf2.region_code.values, round_coords=False):\n    bounds = geom.bounds\n    slices.append((\n        slice(bounds[0], bounds[-2]),\n        slice(bounds[1], bounds[-1])\n    ))", "metadata": {}, "execution_count": null, "outputs": [], "id": "9d2c8123-af5a-4173-b9ba-85d99cbf4adf"}, {"cell_type": "code", "source": "n_partitions = 2\nn_sub_partitions = 2\nblocks = scatter(slices, n_partitions)", "metadata": {}, "execution_count": null, "outputs": [], "id": "2a974ed7-2ad9-49c8-bfbe-a5a466573eb1"}, {"cell_type": "code", "source": "len(blocks), len(blocks[0])", "metadata": {}, "execution_count": null, "outputs": [], "id": "9ad14117-717f-4593-83a1-a8718e1c70fa"}, {"cell_type": "code", "source": "len(scatter(blocks[0], n_sub_partitions)[0])", "metadata": {}, "execution_count": null, "outputs": [], "id": "8eaa1452-6fe8-4c9e-82ed-cfe654426beb"}, {"cell_type": "code", "source": "stats_attrs = [at for at in required_attributes if at not in [\"timestamp\", \"region_code\"]]\nstats_attrs.insert(0, \"Y\")\nstats_attrs.insert(0, \"X\")", "metadata": {}, "execution_count": null, "outputs": [], "id": "42716c4a-c1d2-4d42-bada-f706b058c935"}, {"cell_type": "code", "source": "stats_results = []\ntasks_dict = {stat: [] for stat in stats_attrs}\nreduce_tasks = []\n\nfor i, block in enumerate(blocks):\n    sub_tasks = []\n    sub_blocks = scatter(block, n_sub_partitions)\n\n    for si, sub_block in enumerate(sub_blocks):\n        for attribute in stats_attrs:\n            \n            if attribute in [\"X\", \"Y\"]:\n                schema = attribute\n            else:\n                schema = None\n\n            task_name = f\"block-{i}-sub_block-{si}-{attribute}\"\n            task = Delayed(\"sixy6e/basic_statistics_incremental\", name=task_name)(array_uri, config_dict, attribute, schema=schema, idxs=sub_block, summarise=False)\n\n            if len(tasks_dict[attribute]) > 1:\n                task.depends_on(tasks_dict[attribute][-1])\n\n            tasks_dict[attribute].append(task)\n\nfor attribute in stats_attrs:\n    task_name = f\"reduce-attibute-{attribute}\"\n    reducer_task = Delayed(\"sixy6e/basic_statistics_reduce\", name=task_name)(tasks_dict[attribute], attribute)\n    reduce_tasks.append(reducer_task)\n\ncollect_stats_task = Delayed(gather_stats, local=True, name=\"gather-stats\")(reduce_tasks)", "metadata": {}, "execution_count": null, "outputs": [], "id": "f96ee0d5-daa2-4563-9003-2ea85e8495d6"}, {"cell_type": "code", "source": "stats_results = collect_stats_task.compute()", "metadata": {}, "execution_count": null, "outputs": [], "id": "f1f153d5-f0e5-4801-a0c1-6bc55c604e79"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "3e46cc47-c8b8-4b63-8ba0-40d791621892"}, {"cell_type": "code", "source": "# check the vertical datum", "metadata": {}, "execution_count": null, "outputs": [], "id": "f64c61c1-e23d-4340-9594-3d36ae01f8b8"}, {"cell_type": "code", "source": "asb_metadata", "metadata": {}, "execution_count": null, "outputs": [], "id": "b9a1c11c-52c7-4db5-bf95-f33ba294d68d"}, {"cell_type": "code", "source": "crs_info = {\n    \"horizontal_datum\": \"epsg:4326\",\n    \"vertical_data\": \"epsg:4326\",\n}", "metadata": {}, "execution_count": null, "outputs": [], "id": "4a22741e-57c4-41d2-bdef-394b55f0a684"}, {"cell_type": "code", "source": "with tiledb.open(array_uri, \"w\", ctx=ctx) as ds:\n    ds.meta[\"crs_info\"] = json.dumps(crs_info)\n    ds.meta[\"basic_statistics\"] = json.dumps(stats_results, cls=stac_metadata.Encoder)", "metadata": {}, "execution_count": null, "outputs": [], "id": "3f31ae8f-0044-4132-9c38-616d9c0bade0"}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": [], "id": "5ea6024d-ed0e-46fe-83f2-0716328ebccc"}, {"cell_type": "code", "source": "# produce stac metadata", "metadata": {}, "execution_count": null, "outputs": [], "id": "484f0836-26ed-4391-b422-b9079e473386"}, {"cell_type": "code", "source": "dataset_metadata = stac_metadata.prepare(\n    uid,\n    sonar_metadata,\n    stats_results,\n    asb_metadata,\n    array_uri,\n    coverage_uri,\n    soundings_cell_density_uri,\n    creds.access_key,\n    creds.secret_key,\n    final_start_end_timestamps,\n    outdir_uri,\n    stac_md_uri,\n)", "metadata": {}, "execution_count": null, "outputs": [], "id": "61f1efe4-376f-476c-90d1-4617aec366a8"}]}